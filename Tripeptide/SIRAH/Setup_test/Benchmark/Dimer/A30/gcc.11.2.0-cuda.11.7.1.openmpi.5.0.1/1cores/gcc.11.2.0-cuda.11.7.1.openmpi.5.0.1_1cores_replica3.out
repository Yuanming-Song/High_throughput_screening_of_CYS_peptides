                      :-) GROMACS - gmx mdrun, 2024.2 (-:

Executable:   /opt/apps/gromacs/2024.2/gcc/11.2.0/cuda/11.7.1/openmpi/5.0.1/bin/gmx_mpi
Data prefix:  /opt/apps/gromacs/2024.2/gcc/11.2.0/cuda/11.7.1/openmpi/5.0.1
Working dir:  /dfs9/tw/yuanmis1/mrsec/ML-MD-Peptide/SIRAH/Setup_test/Benchmark/Dimer/A30/gcc.11.2.0-cuda.11.7.1.openmpi.5.0.1/1cores
Command line:
  gmx_mpi mdrun -s /dfs9/tw/yuanmis1/mrsec/ML-MD-Peptide/SIRAH/Setup_test/Benchmark/Dimer/C_A_A_SIRAH_md.tpr -deffnm C_A_A_SIRAH_md_replica3 -nb gpu -pme gpu -bonded gpu -update gpu

Compiled SIMD is AVX2_128, but AVX_512 might be faster (see log).
Reading file /dfs9/tw/yuanmis1/mrsec/ML-MD-Peptide/SIRAH/Setup_test/Benchmark/Dimer/C_A_A_SIRAH_md.tpr, VERSION 2024.2 (single precision)
Changing nstlist from 10 to 25, rlist from 1.225 to 1.398

1 GPU selected for this run.
Mapping of GPU IDs to the 2 GPU tasks in the 1 rank on this node:
  PP:0,PME:0
PP tasks will do (non-perturbed) short-ranged and most bonded interactions on the GPU
PP task will update and constrain coordinates on the GPU
PME tasks will do all aspects on the GPU
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 

starting mdrun 'Protein in water'
10000 steps,    200.0 ps.

Writing final coordinates.

NOTE: 51 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:        4.578        4.579      100.0
                 (ns/day)    (hour/ns)
Performance:     3774.330        0.006

GROMACS reminds you: "Go back to the rock from under which you came" (Fiona Apple)

